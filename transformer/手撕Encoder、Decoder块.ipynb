{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"../imgs/transformer.png\"  width=\"500\" />"
   ],
   "id": "d3ef56d5b6409fa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1 Encoder Block（单层编码器块）",
   "id": "92130a68fbe8457"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-10T15:03:04.583021Z",
     "start_time": "2025-05-10T15:03:03.242646Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 自注意力块\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        # post 层归一化\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        # 前馈层\n",
    "        self.ffn = nn.Sequential(\n",
    "                nn.Linear(in_features=embed_dim, out_features=ffn_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=ffn_hidden_dim, out_features=embed_dim),\n",
    "                nn.Dropout(dropout)\n",
    "        )\n",
    "        # post 层归一化\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 自注意\n",
    "        attn_output, _ = self.self_attn.forward(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                attn_mask=mask\n",
    "        )\n",
    "        x = x + self.dropout(attn_output)\n",
    "        # 层归一化\n",
    "        x = self.ln1(x)\n",
    "        # 前馈层\n",
    "        ffn_out = self.ffn.forward(input=x)\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        # 层归一化\n",
    "        x = self.ln2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 测试",
   "id": "8c336e8d83e69eab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T15:03:05.182681Z",
     "start_time": "2025-05-10T15:03:05.133742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder_block = EncoderBlock(embed_dim=512, num_heads=8, ffn_hidden_dim=2048)\n",
    "x = torch.randn(2, 10, 512)\n",
    "encoder_output = encoder_block(x)\n",
    "encoder_output.shape"
   ],
   "id": "76f878a53a91d412",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2 Decoder Block（单层解码器块）",
   "id": "fa9b248d3561b177"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T15:04:54.614160Z",
     "start_time": "2025-05-10T15:04:54.609159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, attn_heads, ffn_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 带掩码的自注意力\n",
    "        self.masked_self_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=attn_heads, dropout=dropout)\n",
    "        # 层归一化\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        # 交叉注意力\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=attn_heads, dropout=dropout)\n",
    "        # 层归一化\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        # 前馈层\n",
    "        self.ffn = nn.Sequential(\n",
    "                nn.Linear(in_features=embed_dim, out_features=ffn_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_features=ffn_hidden_dim, out_features=embed_dim),\n",
    "                nn.Dropout(dropout)\n",
    "        )\n",
    "        # 层归一化\n",
    "        self.ln3 = nn.LayerNorm(embed_dim)\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        # 带掩码的自注意力\n",
    "        attn_output, _ = self.masked_self_attn.forward(\n",
    "                query=x,\n",
    "                key=x,\n",
    "                value=x,\n",
    "                attn_mask=self_attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(attn_output)\n",
    "        # 层归一化\n",
    "        x = self.ln1(x)\n",
    "        # 交叉注意力\n",
    "        cross_attn_output, _ = self.cross_attn.forward(\n",
    "                query=x,\n",
    "                key=encoder_output,\n",
    "                value=encoder_output,\n",
    "                attn_mask=cross_attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(cross_attn_output)\n",
    "        # 层归一化\n",
    "        x = self.ln2(x)\n",
    "        # 前馈层\n",
    "        x = self.ffn(x)\n",
    "        x = x + self.dropout(x)\n",
    "        # 层归一化\n",
    "        x = self.ln3(x)\n",
    "        return x"
   ],
   "id": "ff3a8fab90dbffbd",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 测试",
   "id": "6343a476c85653b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T15:04:56.290950Z",
     "start_time": "2025-05-10T15:04:56.270968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert encoder_output is not None\n",
    "\n",
    "x = torch.randn(2, 10, 512)\n",
    "decoder_block = DecoderBlock(embed_dim=512, attn_heads=8, ffn_hidden_dim=2048)\n",
    "decoder_output = decoder_block(x, encoder_output)\n",
    "decoder_output.shape"
   ],
   "id": "a1d3742c5dae035b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
